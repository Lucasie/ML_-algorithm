# KNN思想及构建流程
## Knn思想:
    将带有标签的样本数据集(traing/testing),然后将未知的数据输入到模型,将模型与input_data映射到二维空间并计算他们的欧式距离,并从小到大的顺序排列,可以找到K个距离input_data距离最近的电影,我们认为input_data数据属于某个分类(取决于K,K多为奇数取值,一般小于等于20)
## KNN构建流程:
    1,收集数据:任何方法
    2,距离计算:所需要的数值,最好是结构化的数据格式
    3,分析数据:任何方法
    4,训练算法:此步骤不适用于K-临近算法
    5,测试算法:计算错误率(或者准确率)
    6,使用算法:输入样本数据和结构化的输入结果,然后运行K-临近算法判别输入数据分类属于哪一类,最后对计算出的分类执行后续处理.
## KNN算法优缺点:
    优点:精度高,对异常值不敏感,无数据输入假定
    缺点:计算复杂度高/空间复杂度高
    适用范围:数值型和标称型
## KNN所需数学知识点:
    KNN，英文全称为K-nearst neighbor，中文名称为K近邻算法，它是由Cover和Hart在1968年提出来的。 KNN算法流程： 
    输入：训练数据集　T=(x1,y1),(x2,y2),...,(xN,yN)
    其中，xi∈⊆Rn为实例的特征向量，yi∈={c1,c2,...,ck}为实例的类别，i=1,2,...,N；实例特征向量x; 
    输出: 实例x所属的类y
        (1) 根据给点的距离度量，在训练集T中找出与x最近邻的k个点，涵盖着k个点的领域，记为Nk(x); 
    　　(2) 在Nk(x)中根据分类决策规则(如多数表决)，决定x的类别y:     y=argmaxcj∑xi∈Nk(x)I(yi=cj),i=1,2,...,N;
    在上式中，I为指示函数，即当yi=cj时，I为1，否则I为0。 
    KNN特殊情况是k=1的情形，称为最近邻算法。对于输入的实例点(特征向量)x，最近邻算法将训练数据集中与x最近邻点的类作为x
    的类。 
    在KNN算法中，常用的距离有三种，分别为曼哈顿距离、欧式距离和闵可夫斯基距离。 设特征空间是n维实数向量空间Rn
    , xi,xj∈,xi=(x(1)i,x(2)i,...,x(n)i)T
    , xj=(x(1)j,x(2)j,...,x(n)j)T
    , xi,xj
    的Lp
    距离定义为： 
    　　
    Lp(xi,xj)=(∑nl=1|x(l)i−x(l)j|p)1p
    
    　　这里 p≥1
     
    　　当p=1
    时，称为曼哈顿距离(Manhattan distance), 公式为： 
    　　
    L1(xi,xj)=∑nl=1|x(l)i−x(l)j|
    
    　　当p=2
    时，称为欧式距离(Euclidean distance)，即 
    　　
    L2(xi,xj)=(∑nl=1|x(l)i−x(l)j|2)12
    
    　　当p=∞
    时，它是各个坐标距离的最大值，计算公式为： 
    　　
    L∞(xi,xj)=maxl|x(l)i−x(l)j|.
    [KNN](https://blog.csdn.net/sanqima/article/details/51276640)